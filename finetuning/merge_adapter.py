#!/usr/bin/env python3
"""
åˆå¹¶LoRAé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹ï¼Œç”Ÿæˆå®Œæ•´æ¨¡å‹ç”¨äºè¯„ä¼°
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import argparse
from pathlib import Path

def merge_and_save_model(adapter_model_id: str, output_dir: str, push_to_hub: bool = False, hub_model_id: str = None):
    """
    åˆå¹¶adapteråˆ°åŸºç¡€æ¨¡å‹å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹
    
    Args:
        adapter_model_id: HuggingFace adapteræ¨¡å‹ID
        output_dir: è¾“å‡ºç›®å½•
        push_to_hub: æ˜¯å¦ä¸Šä¼ åˆ°HuggingFace
        hub_model_id: HuggingFaceæ¨¡å‹ID
    """
    
    print("ğŸ”§ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        "codellama/CodeLlama-7b-Python-hf",
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
    
    print(f"ğŸ”— æ­£åœ¨åŠ è½½adapter: {adapter_model_id}")
    
    # åŠ è½½PEFTæ¨¡å‹
    model = PeftModel.from_pretrained(base_model, adapter_model_id)
    
    print("ğŸ”„ æ­£åœ¨åˆå¹¶adapteråˆ°åŸºç¡€æ¨¡å‹...")
    
    # åˆå¹¶adapteræƒé‡åˆ°åŸºç¡€æ¨¡å‹
    merged_model = model.merge_and_unload()
    
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    merged_model.save_pretrained(output_dir, safe_serialization=True)
    tokenizer.save_pretrained(output_dir)
    
    print("âœ… æ¨¡å‹åˆå¹¶å®Œæˆ!")
    
    # ä¸Šä¼ åˆ°HuggingFace Hub
    if push_to_hub and hub_model_id:
        print(f"ğŸ“¤ æ­£åœ¨ä¸Šä¼ åˆ°HuggingFace Hub: {hub_model_id}")
        merged_model.push_to_hub(hub_model_id)
        tokenizer.push_to_hub(hub_model_id)
        print("âœ… ä¸Šä¼ å®Œæˆ!")
        return hub_model_id
    
    return output_dir

def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶LoRA adapteråˆ°åŸºç¡€æ¨¡å‹")
    parser.add_argument("--adapter_model_id", type=str, 
                       default="yuexishen/codellama-7b-mbpp-ppo-qlora",
                       help="HuggingFace adapteræ¨¡å‹ID")
    parser.add_argument("--output_dir", type=str, 
                       default="./merged_model",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--push_to_hub", action="store_true",
                       help="ä¸Šä¼ åˆ°HuggingFace Hub")
    parser.add_argument("--hub_model_id", type=str,
                       help="HuggingFace Hubæ¨¡å‹ID")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹æ¨¡å‹åˆå¹¶è¿‡ç¨‹...")
    print(f"ğŸ“‚ Adapteræ¨¡å‹: {args.adapter_model_id}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    if args.push_to_hub and not args.hub_model_id:
        print("âŒ é”™è¯¯: è¦ä¸Šä¼ åˆ°Hubå¿…é¡»æŒ‡å®šhub_model_id")
        return
    
    merged_model_path = merge_and_save_model(
        adapter_model_id=args.adapter_model_id,
        output_dir=args.output_dir,
        push_to_hub=args.push_to_hub,
        hub_model_id=args.hub_model_id
    )
    
    print("ğŸ¯ åˆå¹¶å®Œæˆ! ç°åœ¨å¯ä»¥ç”¨äºè¯„ä¼°:")
    print(f"Model path: {merged_model_path}")
    
    # è¾“å‡ºè¯„ä¼°å‘½ä»¤ç¤ºä¾‹
    print("\nğŸ“‹ BigCodeè¯„ä¼°å‘½ä»¤:")
    if args.push_to_hub and args.hub_model_id:
        model_name = args.hub_model_id
    else:
        model_name = args.output_dir
        
    print(f"""
cd ../bigcode-evaluation-harness
accelerate launch main.py \\
  --model {model_name} \\
  --max_length_generation 512 \\
  --tasks mbpp \\
  --temperature 0.1 \\
  --n_samples 15 \\
  --batch_size 10 \\
  --allow_code_execution
""")

if __name__ == "__main__":
    main() 